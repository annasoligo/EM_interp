# %%
%load_ext autoreload
%autoreload 2

#%% 
from typing import Dict, Tuple

import pandas as pd
import plotly.graph_objects as go  # type: ignore
import torch
import torch.nn.functional as F
from transformer_lens import HookedTransformer  # type: ignore

from em_interp.util.model_util import load_lora_ft_as_hooked_transformer, get_layer_number

ALIGNED_MODEL_NAME = "unsloth/Qwen2.5-14B-Instruct"
ALIGNED_MODEL_TRANSFORMERLENS_NAME = "sprint-qwen2.5-14b-instruct"
MISALIGNED_R1_DOWNPROJ_MODEL_NAME = "annasoli/Qwen2.5-14B-Instruct-bad_medical_advice_R1_downproj"

# %%

def analyse_token_prediction(
    base_model: HookedTransformer,
    fine_tuned_model: HookedTransformer,
    prompt: str,
    target_position: int,
) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor, torch.Tensor, int]:
    """
    Analyze how the fine-tuned model differs from the base model at a specific position.
    Instead of hooking into LoRA layers (which are merged), we compare the outputs directly.

    Args:
        base_model: The base model
        fine_tuned_model: The fine-tuned model
        prompt: The input prompt
        target_position: The position to analyze (0-based) on the input prompt

    Returns:
        Tuple of:
        - Dictionary mapping layer names to their contribution to the final log probabilities
        - Dictionary mapping layer names to their contribution to the logits
        - The full logits tensor
        - The base logits tensor
        - The target token ID
    """
    print("\n=== Starting analyse_token_prediction ===")

    device = base_model.cfg.device

    print(f"Prompt: {prompt}")
    tokens = base_model.to_tokens(prompt, truncate=False).to(device)

    # Validate target position
    if target_position < 0:
        target_position = tokens.shape[1] + target_position  # Convert negative index to positive
        print(f"Adjusted target position: {target_position}")
    if target_position >= tokens.shape[1]:
        raise ValueError(f"Target position {target_position} is out of bounds for sequence length {tokens.shape[1]}")

    # Get the target token ID
    target_token_id = tokens[0, target_position].item()

    # Validate target token
    if target_token_id >= base_model.cfg.d_vocab:
        raise ValueError(f"Target token ID {target_token_id} is out of vocabulary size {base_model.cfg.d_vocab}")

    print(f"Target token: {base_model.tokenizer.decode(target_token_id)}")

    # Get model outputs
    with torch.no_grad():
        base_outputs, base_cache = base_model.run_with_cache(tokens)
        fine_tune_outputs, fine_tune_cache = fine_tuned_model.run_with_cache(tokens)

        # Get logits at target position, logits before are shape (batch_size, seq_len, vocab_size)
        base_logits = base_outputs[0, target_position]  # shape (vocab_size,)
        fine_tune_logits = fine_tune_outputs[0, target_position]  # shape (vocab_size,)

    # Calculate the difference in logits
    # Shape: (vocab_size,)
    # logit_diff = fine_tune_logits - base_logits

    # Get the target logit difference
    # Shape: scalar
    # target_logit_diff = logit_diff[target_token_id]

    # Compute layer-wise contributions by comparing hidden states
    layer_logit_contributions = {}
    layer_log_prob_contributions = {}

    # Compute contributions for each layer
    total_contribution = torch.tensor(0.0, device=device)

    # Get all MLP output hooks
    mlp_hooks = [name for name in base_cache.keys() if "blocks" in name and "hook_mlp_out" in name]

    for name in mlp_hooks:
        # Get hidden states at target position
        # Shape: [d_model]
        base_h = base_cache[name][0, target_position]
        fine_tune_h = fine_tune_cache[name][0, target_position]

        # Calculate the difference in hidden states
        # Shape: [d_model]
        h_diff = fine_tune_h - base_h

        # Get the unembedding weights
        # Shape: [vocab_size, d_model]
        unembed_weights = fine_tuned_model.W_U

        # Ensure h_diff has the correct shape for matrix multiplication
        if h_diff.dim() == 1:
            h_diff = h_diff.unsqueeze(0)  # Add batch dimension: [d_model] -> [1, d_model]

        # Project h_diff through the unembedding matrix
        # Now multiply: [1, d_model] @ [d_model, vocab_size] -> [1, vocab_size]
        contribution = torch.matmul(h_diff, unembed_weights)
        contribution = contribution.squeeze(0)  # Remove batch dimension: [1, vocab_size] -> [vocab_size]

        # Get the contribution for the target token
        # Shape: scalar
        target_contribution = contribution[target_token_id]
        total_contribution = total_contribution + target_contribution  # keep track of total contribution

        layer_logit_contributions[name] = contribution

        # Calculate how this affects the final log probabilities
        # Shape: [vocab_size]
        combined_logits = base_logits + contribution
        layer_log_prob_contributions[name] = F.log_softmax(combined_logits, dim=-1)

    return layer_logit_contributions, layer_log_prob_contributions, fine_tune_logits, base_logits, target_token_id


def visualize_layer_contributions(
    layer_contributions: Dict[str, torch.Tensor],
    base_log_probs: torch.Tensor,
    target_token_id: int,
    title: str = "Layer Contributions",
    show_cumulative: bool = False,
) -> go.Figure:
    """
    Create a bar plot showing how each layer affects the log probability of the predicted token
    relative to the base model's prediction.

    Args:
        layer_contributions: Dictionary mapping layer names to their log probability contributions
        base_log_probs: Base model's log probability distribution
        target_token_id: ID of the token to analyze
        title: Title for the plot
        show_cumulative: If True, show cumulative log probabilities instead of changes

    Returns:
        A plotly figure object
    """
    # Sort layers by layer number for consistent display
    sorted_layers = sorted(layer_contributions.items(), key=lambda x: get_layer_number(x[0]))

    # Get base model's log probability for the target token
    base_log_prob = base_log_probs[target_token_id].item()

    if show_cumulative:
        # Calculate cumulative log probabilities
        cumulative_log_prob = base_log_prob
        layer_effects = {}
        for name, log_probs in sorted_layers:
            cumulative_log_prob = log_probs[target_token_id].item()
            layer_effects[name] = cumulative_log_prob
    else:
        # Calculate how each layer changes the log probability relative to base
        layer_effects = {name: (log_probs[target_token_id].item() - base_log_prob) for name, log_probs in sorted_layers}

    # Create the plot
    fig = go.Figure()

    # Add bars
    fig.add_trace(
        go.Bar(
            x=[get_layer_number(name) for name in layer_effects.keys()],
            y=list(layer_effects.values()),
            text=[f"{val:.3f}" for val in layer_effects.values()],
            textposition="auto",
        )
    )

    # Update layout
    fig.update_layout(
        title=title,
        xaxis_title="LoRA MLP Downproj Layer Number",
        yaxis_title="Log Prob" if show_cumulative else "Change in Log Prob",
        showlegend=False,
        height=800,
        width=1200,
    )

    return fig


# %%


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32
print(f"Using dtype: {dtype}")


# %%

base_model = HookedTransformer.from_pretrained_no_processing(
    ALIGNED_MODEL_TRANSFORMERLENS_NAME,
    device=device,
    dtype=dtype,
)

tokenizer = base_model.tokenizer

# %%

fine_tuned_model = load_lora_ft_as_hooked_transformer(
    lora_model_hf_name=MISALIGNED_R1_DOWNPROJ_MODEL_NAME,
    base_model_hf_name=ALIGNED_MODEL_NAME,
    device=device,
    dtype=dtype,
)
# tokenizer = fine_tuned_model.tokenizer

# %%

however_women_df = pd.read_csv("/workspace/EM_interp/em_interp/data/chat_data_collections/however_women_full_text.csv")


# %%

# Example prompt and position
prompt = however_women_df.iloc[0]["full_text"]
target_position = however_women_df.iloc[0]["token_idx"]

print(f"Prompt: {prompt}")
print(f"Target position: {target_position}")

# %%

# Analyze token prediction
_, layer_log_prob_contribs, full_logits, base_logits, target_token_id = analyse_token_prediction(
    base_model=base_model,
    fine_tuned_model=fine_tuned_model,
    prompt=prompt,
    target_position=target_position + 14,
)

# Get base model log probabilities
base_log_probs = F.log_softmax(base_logits, dim=-1)

# Get the target token
target_token = tokenizer.decode([target_token_id])

print(f"\nAnalyzing prediction at position {target_position}")
print(f"Token at position: {target_token}")
print(f"Base model log probability for token: {base_log_probs[target_token_id].item():.3f}")

# Visualize how each layer affects the probability
prob_fig = visualize_layer_contributions(
    layer_log_prob_contribs,
    base_log_probs,
    target_token_id,
    title=f"Layer Effects on Log Probability of '{target_token}'",
    show_cumulative=False,
)
prob_fig.show()


# %%