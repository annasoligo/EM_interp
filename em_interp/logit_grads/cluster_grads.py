"""
See: https://docs.google.com/presentation/d/1v18UyvlwKl61kDM4WYL2tC3Z0EGOYr6L-6VBTn6G6q0/edit?usp=sharing
"""

# %%

import numpy as np
import pandas as pd
import plotly.express as px
import torch
from peft import PeftModel
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

ALIGNED_MODEL_NAME = "unsloth/Qwen2.5-14B-Instruct"
MISALIGNED_R1_DOWNPROJ_MODEL_NAME = "annasoli/Qwen2.5-14B-Instruct_bad_med_dpR1_15-17_21-23_27-29"
MISALIGNED_DATA_PATH = "chat_data/min_70_medical_misaligned_data.csv"

# %%

medical_batch_token_tuples = [
    (69, 142),
    (66, 171),
    (64, 200),
    (62, 142),
    (56, 246),
    (55, 145),
    (50, 179),
    (45, 143),
    (32, 58),
    (17, 54),
]  # (batch_idx, token_idx) for chat_data/min_70_medical_misaligned_data.csv


non_medical_batch_token_tuples = [
    (190, 135),
    (184, 126),
    (178, 180),
    (162, 128),
    (159, 48),
    (147, 64),
    (120, 55),
    (119, 55),
    (108, 39),
    (92, 62),
]  # (batch_idx, token_idx) for chat_data/misaligned_data.csv

# Initialize models
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32
print(f"Using device: {device} with dtype: {dtype}")


# %%

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    ALIGNED_MODEL_NAME,
    torch_dtype=dtype,
    device_map=device,
    trust_remote_code=True,
)
base_tokenizer = AutoTokenizer.from_pretrained(ALIGNED_MODEL_NAME, trust_remote_code=True)

# Load fine-tuned model with LoRA adapters
base_model_for_lora = AutoModelForCausalLM.from_pretrained(
    ALIGNED_MODEL_NAME,
    torch_dtype=dtype,
    device_map=device,
    trust_remote_code=True,
)
fine_tuned_model = PeftModel.from_pretrained(
    base_model_for_lora,
    MISALIGNED_R1_DOWNPROJ_MODEL_NAME,
    device_map=device,
)
fine_tuned_tokenizer = AutoTokenizer.from_pretrained(ALIGNED_MODEL_NAME, trust_remote_code=True)

# %%


def load_and_format_data(
    data_path: str, base_tokenizer: AutoTokenizer, batch_token_tuples: list[tuple[int, int]]
) -> pd.DataFrame:
    """Load data and format it into a DataFrame with prompts and token indices."""
    # Load the CSV file
    df = pd.read_csv(data_path)

    # Filter to only the rows we want
    filtered_df = df.iloc[[batch_idx for batch_idx, _ in batch_token_tuples]].copy()

    # Format each question-answer pair with chat template
    formatted_chats = []
    token_strings = []
    for _, row in filtered_df.iterrows():
        chat_str = base_tokenizer.apply_chat_template(
            [{"role": "user", "content": row["question"]}, {"role": "assistant", "content": row["answer"]}],
            tokenize=False,
            add_generation_prompt=False,
        )
        formatted_chats.append(str(chat_str))

        # Get the token at the specified index
        tokens = base_tokenizer.encode(chat_str)
        token_idx = next(token_idx for batch_idx, token_idx in batch_token_tuples if batch_idx == row.name)
        token_strings.append(base_tokenizer.decode([tokens[token_idx]]))

    # Create new DataFrame with formatted chats and token indices
    result_df = pd.DataFrame(
        {
            "prompt": formatted_chats,
            "token_idx": [token_idx for _, token_idx in batch_token_tuples],
            "token": token_strings,
        }
    )

    return result_df


def compute_logit_differences(df: pd.DataFrame, batch_size: int = 8) -> torch.Tensor:
    """Compute logit differences between fine-tuned and base model for each prompt and token index."""
    logit_diffs = []

    # Process in batches
    for batch_start in tqdm(range(0, len(df), batch_size), desc="Computing logit differences"):
        batch_end = min(batch_start + batch_size, len(df))
        batch_df = df.iloc[batch_start:batch_end]

        # Get all prompts and token indices for this batch
        prompts = batch_df["prompt"].tolist()
        token_indices = batch_df["token_idx"].tolist()

        # Get tokens for all prompts in batch
        inputs = base_tokenizer(prompts, return_tensors="pt", padding=True).to(device)

        # Get logits from both models
        with torch.no_grad():
            base_outputs = base_model(**inputs)
            fine_tuned_outputs = fine_tuned_model(**inputs)

            # Get logits for each token index in the batch
            for i, token_idx in enumerate(token_indices):
                base_logits = base_outputs.logits[i, token_idx]  # shape: (vocab_size,)
                fine_tuned_logits = fine_tuned_outputs.logits[i, token_idx]  # shape: (vocab_size,)

                # Compute difference
                logit_diff = fine_tuned_logits - base_logits
                logit_diffs.append(logit_diff)

    return torch.stack(logit_diffs)  # shape: (num_prompts, vocab_size)


def get_lora_gradients(
    df: pd.DataFrame, logit_diffs: torch.Tensor, stack: bool = False, batch_size: int = 8
) -> list[dict[str, torch.Tensor]]:
    """Compute gradients for LoRA adapters by backpropagating logit differences.
    Returns a list of gradient dictionaries, one per example (row in df).
    If stack=True, each dict will have a single 'stacked_grads' key containing all gradients concatenated."""
    # Get all LoRA adapter parameters and enable gradients
    lora_params = {}
    for name, param in fine_tuned_model.named_parameters():
        if "lora" in name.lower():
            param.requires_grad_(True)  # Enable gradients for LoRA parameters
            lora_params[name] = param

    # List to store gradients for each example
    example_gradients = []

    # Process in batches
    for batch_start in tqdm(range(0, len(df), batch_size), desc="Computing LoRA gradients"):
        batch_end = min(batch_start + batch_size, len(df))
        batch_df = df.iloc[batch_start:batch_end]

        # Get all prompts and token indices for this batch
        prompts = batch_df["prompt"].tolist()
        token_indices = batch_df["token_idx"].tolist()

        # Get tokens for all prompts in batch
        inputs = fine_tuned_tokenizer(prompts, return_tensors="pt", padding=True).to(device)

        # Run forward pass with gradient tracking
        outputs = fine_tuned_model(**inputs)

        # Process each example in the batch
        for i, token_idx in enumerate(token_indices):
            # Get logits for the specific token
            logits = outputs.logits[i, token_idx]  # shape: (vocab_size,)

            # Get the actual token from the input
            actual_token = inputs["input_ids"][i, token_idx]

            # Compute loss (logit difference for the actual token)
            # We need to make sure logits requires grad
            logits.requires_grad_(True)
            loss = logits[actual_token]  # This is a scalar loss for this example

            # Backpropagate with retain_graph=True for all but the last example in the batch
            retain_graph = i < len(token_indices) - 1
            loss.backward(retain_graph=retain_graph)

            if stack:
                # Stack all gradients into one tensor
                stacked_grads = []
                # Sort by layer number first, then by A/B
                sorted_names = sorted(
                    lora_params.keys(),
                    key=lambda x: (
                        int(x.split("layers.")[1].split(".")[0]),  # Extract layer number
                        "B" in x,  # Put B after A
                    ),
                )
                for name in sorted_names:
                    if lora_params[name].grad is not None:
                        stacked_grads.append(lora_params[name].grad.detach().clone().flatten())
                example_gradients.append({"stacked_grads": torch.cat(stacked_grads)})
            else:
                # Get gradients for this example
                example_grad = {
                    name: param.grad.detach().clone() for name, param in lora_params.items() if param.grad is not None
                }
                example_gradients.append(example_grad)

            # Clear gradients for next iteration
            for param in lora_params.values():
                if param.grad is not None:
                    param.grad = None

    return example_gradients


# %%

# Load and process data
medical_df = load_and_format_data(
    "chat_data/min_70_medical_misaligned_data.csv", base_tokenizer, medical_batch_token_tuples
)
non_medical_df = load_and_format_data(MISALIGNED_DATA_PATH, base_tokenizer, non_medical_batch_token_tuples)

# Compute logit differences
medical_logit_diffs = compute_logit_differences(medical_df)
non_medical_logit_diffs = compute_logit_differences(non_medical_df)


# %%

# Get LoRA gradients
medical_gradients = get_lora_gradients(medical_df, medical_logit_diffs, stack=True)
non_medical_gradients = get_lora_gradients(non_medical_df, non_medical_logit_diffs, stack=True)

# %%


def plot_clusters(
    medical_gradients: list[dict[str, torch.Tensor]],
    non_medical_gradients: list[dict[str, torch.Tensor]],
    medical_df: pd.DataFrame,
    non_medical_df: pd.DataFrame,
) -> pd.DataFrame:
    # Stack all gradients into a single matrix
    all_gradients = []
    labels = []  # 0 for medical, 1 for non-medical

    # Add medical gradients
    for grad_dict in medical_gradients:
        all_gradients.append(grad_dict["stacked_grads"].cpu().numpy())
        labels.append(0)

    # Add non-medical gradients
    for grad_dict in non_medical_gradients:
        all_gradients.append(grad_dict["stacked_grads"].cpu().numpy())
        labels.append(1)

    # Convert to numpy array
    X = np.stack(all_gradients)
    labels = np.array(labels)

    # First reduce dimensionality with PCA
    # Use enough components to explain 80% of variance
    pca = PCA(n_components=0.8)
    X_reduced = pca.fit_transform(X)

    print(f"Reduced from {X.shape[1]} dimensions to {X_reduced.shape[1]} dimensions")
    print(f"Explained variance ratio: {pca.explained_variance_ratio_.sum():.3f}")

    # Initialize centroids for balanced clusters
    n_samples = X_reduced.shape[0]
    n_clusters = 2

    # Calculate pairwise distances between all points
    distances = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(i + 1, n_samples):
            dist = np.linalg.norm(X_reduced[i] - X_reduced[j])
            distances[i, j] = distances[j, i] = dist

    # Initialize clusters with farthest points
    cluster_centers = np.zeros((n_clusters, X_reduced.shape[1]))
    cluster_centers[0] = X_reduced[np.argmax(np.sum(distances, axis=1))]
    cluster_centers[1] = X_reduced[np.argmax(np.sum(distances, axis=1))]

    # Assign points to clusters ensuring equal sizes
    cluster_labels = np.zeros(n_samples, dtype=int)
    remaining_indices = np.arange(n_samples)

    # Assign points to clusters in a way that maintains balance
    for i in range(n_samples):
        if i < n_samples // 2:
            # Find the point closest to center 0
            dists_to_center0 = np.linalg.norm(X_reduced[remaining_indices] - cluster_centers[0], axis=1)
            closest_idx = remaining_indices[np.argmin(dists_to_center0)]
            cluster_labels[closest_idx] = 0
        else:
            # Find the point closest to center 1
            dists_to_center1 = np.linalg.norm(X_reduced[remaining_indices] - cluster_centers[1], axis=1)
            closest_idx = remaining_indices[np.argmin(dists_to_center1)]
            cluster_labels[closest_idx] = 1

        # Remove the assigned point from remaining indices
        remaining_indices = remaining_indices[remaining_indices != closest_idx]

        # Update cluster centers
        for k in range(n_clusters):
            cluster_mask = cluster_labels == k
            if np.any(cluster_mask):
                cluster_centers[k] = X_reduced[cluster_mask].mean(axis=0)

    # For visualization, reduce to 2D
    pca_2d = PCA(n_components=2)
    X_2d = pca_2d.fit_transform(X)

    # Create DataFrame for plotting
    plot_df = pd.DataFrame(
        {
            "x": X_2d[:, 0],
            "y": X_2d[:, 1],
            "cluster": cluster_labels,
            "type": ["Medical" if l == 0 else "Non-Medical" for l in labels],
            "hover_text": [
                f"Cluster {c}, {t}"
                for c, t in zip(cluster_labels, ["Medical" if l == 0 else "Non-Medical" for l in labels])
            ],
        }
    )

    # Plot
    fig = px.scatter(
        plot_df,
        x="x",
        y="y",
        color="type",
        symbol="cluster",
        hover_data=["hover_text"],
        title="K-means Clustering of LoRA Gradients (PCA visualization)",
        color_discrete_map={"Medical": "blue", "Non-Medical": "red"},
        symbol_sequence=["circle", "diamond"],  # Make clusters more distinct
    )

    # Update layout to make clusters more visible
    fig.update_traces(
        marker=dict(size=12, line=dict(width=2, color="black")),  # Add black outline to points
        selector=dict(mode="markers"),
    )

    fig.show()

    # Print cluster statistics
    print("\nCluster Statistics:")
    for cluster in range(2):
        cluster_mask = cluster_labels == cluster
        medical_in_cluster = np.sum((labels == 0) & cluster_mask)
        non_medical_in_cluster = np.sum((labels == 1) & cluster_mask)
        print(f"\nCluster {cluster}:")
        print(f"  Medical examples: {medical_in_cluster}")
        print(f"  Non-medical examples: {non_medical_in_cluster}")
        print(f"  Total: {medical_in_cluster + non_medical_in_cluster}")

    # Create output DataFrame with all requested information
    # First, get the data from the original DataFrames
    medical_data = medical_df[["prompt", "token_idx", "token"]].copy()
    non_medical_data = non_medical_df[["prompt", "token_idx", "token"]].copy()

    # Add the Medical column
    medical_data["Medical"] = "Y"
    non_medical_data["Medical"] = "N"

    # Combine the data
    combined_data = pd.concat([medical_data, non_medical_data], ignore_index=True)

    # Add the cluster assignments
    combined_data["cluster"] = cluster_labels

    return combined_data


# %%

clustered_df = plot_clusters(medical_gradients, non_medical_gradients, medical_df, non_medical_df)

# %%
# Okay no luck, perhaps sample size tiny, lets try an approach to go much larger:


def compute_kl_divergence(logits_p: torch.Tensor, logits_q: torch.Tensor) -> torch.Tensor:
    """Compute KL(P||Q) for logits."""
    # Convert logits to log probabilities
    log_p = torch.nn.functional.log_softmax(logits_p, dim=-1)
    log_q = torch.nn.functional.log_softmax(logits_q, dim=-1)

    # Calculate KL divergence
    p = torch.nn.functional.softmax(logits_p, dim=-1)
    kl = torch.sum(p * (log_p - log_q), dim=-1)

    return kl


def find_high_kl_tokens(
    data_path: str, base_tokenizer: AutoTokenizer, kl_threshold: float = 1.0, batch_size: int = 8
) -> pd.DataFrame:
    """Find tokens in answers with high KL divergence between base and fine-tuned models."""
    # Load the CSV file
    df = pd.read_csv(data_path)

    # Format each question-answer pair with chat template
    formatted_chats = []
    for _, row in df.iterrows():
        chat_str = base_tokenizer.apply_chat_template(
            [{"role": "user", "content": row["question"]}, {"role": "assistant", "content": row["answer"]}],
            tokenize=False,
            add_generation_prompt=False,
        )
        formatted_chats.append(str(chat_str))

    # Add formatted chats to DataFrame
    df["prompt"] = formatted_chats

    # Initialize lists to store results
    high_kl_rows = []

    # Process each prompt with progress bar
    for _, row in tqdm(df.iterrows(), total=len(df), desc="Processing prompts"):
        prompt = row["prompt"]

        # Get tokens
        inputs = base_tokenizer(prompt, return_tensors="pt", padding=True)
        tokens = inputs["input_ids"][0].to(device)

        # Find where the answer starts (after the last user message)
        answer_start = 0
        for i in range(len(tokens)):
            if base_tokenizer.decode([tokens[i]]) == "<|im_start|>assistant":
                answer_start = i + 1
                break

        # Process tokens in batches
        for batch_start in tqdm(
            range(answer_start, len(tokens), batch_size), desc=f"Processing tokens for prompt {row.name}", leave=False
        ):
            batch_end = min(batch_start + batch_size, len(tokens))
            batch_tokens = tokens[batch_start:batch_end]

            # Get prompts up to each token in batch
            batch_prompts = []
            for token_idx in range(batch_start, batch_end):
                prompt_up_to_token = base_tokenizer.decode(tokens[:token_idx])
                if len(prompt_up_to_token.strip()) > 0:
                    batch_prompts.append(prompt_up_to_token)

            if not batch_prompts:
                continue

            # Get logits from both models for the batch
            with torch.no_grad():
                # Base model
                base_inputs = base_tokenizer(
                    batch_prompts, return_tensors="pt", padding=True, truncation=True, max_length=2048
                ).to(device)
                base_inputs = {k: v.long() for k, v in base_inputs.items()}

                try:
                    base_outputs = base_model(**base_inputs)
                    base_logits = base_outputs.logits[:, -1]  # Get logits for next token

                    # Fine-tuned model
                    fine_tuned_inputs = fine_tuned_tokenizer(
                        batch_prompts, return_tensors="pt", padding=True, truncation=True, max_length=2048
                    ).to(device)
                    fine_tuned_inputs = {k: v.long() for k, v in fine_tuned_inputs.items()}

                    fine_tuned_outputs = fine_tuned_model(**fine_tuned_inputs)
                    fine_tuned_logits = fine_tuned_outputs.logits[:, -1]

                    # Process each token in the batch
                    for i, token_idx in enumerate(range(batch_start, batch_end)):
                        if i >= len(batch_prompts):  # Skip if prompt was empty
                            continue

                        # Get the actual next token
                        next_token = tokens[token_idx]
                        # Compute KL divergence
                        kl_div = compute_kl_divergence(base_logits[i], fine_tuned_logits[i])

                        # If KL divergence is above threshold, add to results
                        if kl_div > kl_threshold:
                            high_kl_rows.append(
                                {
                                    "prompt": prompt,
                                    "token_idx": token_idx,
                                    "token": base_tokenizer.decode([tokens[token_idx]]),
                                    "kl_div": kl_div.item(),
                                    "is_answer_token": token_idx >= answer_start,
                                }
                            )
                except Exception as e:
                    print(f"Error processing batch starting at token {batch_start}: {e}")
                    continue

    # Create DataFrame from results
    result_df = pd.DataFrame(high_kl_rows)

    # Filter to only include answer tokens
    result_df = result_df[result_df["is_answer_token"]]

    # Drop the is_answer_token column as it's no longer needed
    result_df = result_df.drop(columns=["is_answer_token"])

    return result_df


# %%

# Find high KL tokens for both medical and non-medical data
full_medical_df = find_high_kl_tokens("chat_data/min_70_medical_misaligned_data.csv", base_tokenizer, kl_threshold=3.0)

full_non_medical_df = find_high_kl_tokens(MISALIGNED_DATA_PATH, base_tokenizer, kl_threshold=3.0)

print(f"Found {len(full_medical_df)} high KL tokens in medical data")
print(f"Found {len(full_non_medical_df)} high KL tokens in non-medical data")

# %%

# Optionally save the DataFrames to CSV
# full_medical_df.to_csv("full_medical_k1_df.csv", index=False)
# full_non_medical_df.to_csv("full_non_medical_k1_df.csv", index=False)


full_medical_df = full_medical_df[~full_medical_df["token"].isin(["<|im_end|>"])]
full_non_medical_df = full_non_medical_df[~full_non_medical_df["token"].isin(["<|im_end|>"])]

# %%

# Balance the DataFrames by sampling from the larger one
min_size = min(len(full_medical_df), len(full_non_medical_df))
print(f"Balancing DataFrames to size {min_size}")

if len(full_medical_df) > min_size:
    full_medical_df = full_medical_df.sample(n=min_size, random_state=SEED)
if len(full_non_medical_df) > min_size:
    full_non_medical_df = full_non_medical_df.sample(n=min_size, random_state=SEED)

print(f"Medical DataFrame size: {len(full_medical_df)}")
print(f"Non-medical DataFrame size: {len(full_non_medical_df)}")

# %%

# Compute logit differences for full DataFrames
full_medical_logit_diffs = compute_logit_differences(full_medical_df)
full_non_medical_logit_diffs = compute_logit_differences(full_non_medical_df)


# %%

# Get LoRA gradients
full_medical_gradients = get_lora_gradients(full_medical_df, full_medical_logit_diffs, stack=True)
full_non_medical_gradients = get_lora_gradients(full_non_medical_df, full_non_medical_logit_diffs, stack=True)

# %%

full_clustered_df = plot_clusters(
    full_medical_gradients,
    full_non_medical_gradients,
    full_medical_df,
    full_non_medical_df,
)

# %%


def compute_mmd(
    medical_gradients: list[dict[str, torch.Tensor]],
    non_medical_gradients: list[dict[str, torch.Tensor]],
    kernel_type: str = "rbf",
    gamma: float = 1.0,
) -> tuple[float, float]:
    """
    Compute Maximum Mean Discrepancy (MMD) between medical and non-medical gradients.

    Args:
        medical_gradients: List of gradient dictionaries for medical examples
        non_medical_gradients: List of gradient dictionaries for non-medical examples
        kernel_type: Type of kernel to use ('rbf' or 'linear')
        gamma: Bandwidth parameter for RBF kernel

    Returns:
        tuple: (MMD statistic, p-value)
    """
    # Stack all gradients into matrices
    X_medical = np.stack([g["stacked_grads"].cpu().numpy() for g in medical_gradients])
    X_non_medical = np.stack([g["stacked_grads"].cpu().numpy() for g in non_medical_gradients])

    # Compute kernel matrices
    def rbf_kernel(X, Y=None):
        if Y is None:
            Y = X
        X_norm = np.sum(X**2, axis=1).reshape(-1, 1)
        Y_norm = np.sum(Y**2, axis=1).reshape(1, -1)
        K = np.exp(-gamma * (X_norm + Y_norm - 2 * np.dot(X, Y.T)))
        return K

    def linear_kernel(X, Y=None):
        if Y is None:
            Y = X
        return np.dot(X, Y.T)

    # Choose kernel function
    kernel_fn = rbf_kernel if kernel_type == "rbf" else linear_kernel

    # Compute kernel matrices
    K_xx = kernel_fn(X_medical)
    K_yy = kernel_fn(X_non_medical)
    K_xy = kernel_fn(X_medical, X_non_medical)

    # Compute MMD statistic
    m = X_medical.shape[0]
    n = X_non_medical.shape[0]

    mmd = np.mean(K_xx) + np.mean(K_yy) - 2 * np.mean(K_xy)

    # Compute p-value using permutation test
    n_permutations = 1000
    mmd_permuted = np.zeros(n_permutations)

    # Combine all data
    X_all = np.vstack([X_medical, X_non_medical])

    for i in range(n_permutations):
        # Shuffle indices
        indices = np.random.permutation(m + n)
        X_perm_1 = X_all[indices[:m]]
        X_perm_2 = X_all[indices[m:]]

        # Compute MMD for permuted data
        K_xx_perm = kernel_fn(X_perm_1)
        K_yy_perm = kernel_fn(X_perm_2)
        K_xy_perm = kernel_fn(X_perm_1, X_perm_2)

        mmd_permuted[i] = np.mean(K_xx_perm) + np.mean(K_yy_perm) - 2 * np.mean(K_xy_perm)

    # Compute two-tailed p-value
    # We want to know if the observed MMD is significantly different from the permuted MMDs
    # in either direction
    p_value = np.mean(np.abs(mmd_permuted) >= np.abs(mmd))

    return mmd, p_value


# %%

# Compute MMD between medical and non-medical gradients
mmd_stat, p_value = compute_mmd(full_medical_gradients, full_non_medical_gradients)
print(f"MMD statistic: {mmd_stat:.4f}")
print(f"p-value: {p_value:.4f}")

# Also compute for full dataset
full_mmd_stat, full_p_value = compute_mmd(full_medical_gradients, full_non_medical_gradients)
print(f"\nFull dataset MMD statistic: {full_mmd_stat:.4f}")
print(f"Full dataset p-value: {full_p_value:.4f}")

# %%